
Q1. 로지스틱 회귀는 왜 "회귀"라는 이름이 붙었지만 분류에 사용될 수 있을까?
1. 선형 회귀 방식을 사용하므로
2. 확률 값을 출력하는 함수이기 때문
3. 회귀 계수를 사용해서 예측하므로
4. 정답이 연속형 수치이기 때문에


Q2. 로지스틱 회귀에서 사용하는 시그모이드 함수의 역할은 무엇인가요?(선형방정식 -logit을 키워드로 사용하여 답변하기)


Q3. 확률적 경사 하강법(SGD)에서 전체 데이터를 사용하지 않고 1개씩 사용하는 이유는?
(복수 선택 가능)
A. 계산 속도 향상
B. 과대적합 방지
C. 메모리 절약
D. 높은 정확도 보장


Q4. 분류 문제에서 정확도를 손실 함수로 사용할 수 없는 이유는?


Q5. 로지스틱 손실 함수는 타겟 값에 따라 어떤 수식을 사용하나요? 로그계산식(확률은 x로 한다)

타겟 값	손실 함수 식 (y=1 or y=0 기준)
y = 1	 : _____
y = 0	 : _____



Q6. 아래 boolean indexing 코드는 무엇을 의미하나요?

bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')



Q7. partial_fit()과 fit()의 차이점 2가지 설명하세요.


Q8. scikit-learn의 로지스틱 회귀에서 다중 분류를 수행할 수 있는 이유는?


Q9. 다음 코드 실행 결과에서 7개의 확률 합은 얼마인가?

from scipy.special import softmax
decision = lr.decision_function(test_scaled[:1])
proba = softmax(decision, axis=1)
print(proba)
1. 0.7
2. 1.0
3. 클래스 수에 따라 다름
4. 알 수 없음


Q10. 조기 종료(Early Stopping)의 주요 목적은?

1. 테스트 데이터의 정답률을 최대화하기 위해
2. 학습 속도를 빠르게 하기 위해
3. 과대적합을 방지하고 일반화 성능을 높이기 위해
4. 데이터 누락을 방지하기 위해





----------------------------------------------
답
1.  3
2. 시그모이드 함수는 **선형 결합 결과(logit)**를 0~1 사이의 확률값으로 변환해주는 역할
3. a,b,c
4.정확도는 이산적(0 또는 1) 값만 가지므로 미분이 불가능하여 경사하강법에 사용할 수 없음
5. -log(x), -log(1-x)
6. 'Bream' 또는 'Smelt'인 샘플만 True로 설정해서 이진분류로 추출하기 위해
7. partial_fit()은 모델을 초기화하지 않고 기존 모델에 이어서 학습
8. OvR 메소드로 클래스 별 이진분류를 여러번 시행할 수 있어서
9. 2번, 모든 사건의 확률 총합은 1
10. 3
